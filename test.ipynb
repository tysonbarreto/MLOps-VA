{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'VA'\n",
    "\n",
    "list_of_files = [\n",
    "    f\"{PROJECT_NAME}/__init__.py\",\n",
    "    f\"{PROJECT_NAME}/components/__init__.py\",\n",
    "    f\"{PROJECT_NAME}/components/data_ingestion.py\",\n",
    "    f\"{PROJECT_NAME}/components/data_transformation.py\",\n",
    "    f\"{PROJECT_NAME}/components/data_validation.py\",\n",
    "    f\"{PROJECT_NAME}/components/model_trainer.py\",\n",
    "    f\"{PROJECT_NAME}/components/model_evaluation.py\",\n",
    "    f\"{PROJECT_NAME}/entity/__init__.py\",\n",
    "    f\"{PROJECT_NAME}/config/__init__.py\",\n",
    "    f\"{PROJECT_NAME}/config/configuration.py\",\n",
    "    f\"{PROJECT_NAME}/utils/__init__.py\",\n",
    "    f\"{PROJECT_NAME}/utils/utils.py\",\n",
    "    f\"{PROJECT_NAME}/exception/__init__.py\",\n",
    "    f\"{PROJECT_NAME}/logger/__init__.py\",\n",
    "    \"app.py\",\n",
    "    \"requirements.txt\",\n",
    "    \"Dockerfile\",\n",
    "    \".dockerignore\",\n",
    "    \"setup.py\",\n",
    "    \"config/model.yaml\",\n",
    "    \"config/schema.yaml\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in list_of_files:\n",
    "    filepath = Path(files)\n",
    "    filedir, filename = os.path.split(filepath)\n",
    "    \n",
    "    if filedir !=\"\":\n",
    "        os.makedirs(filedir, exist_ok=True)\n",
    "    if (not os.path.exists(filepath)) or (os.path.getsize(filepath)==0):\n",
    "        with open(filepath,'w') as f:\n",
    "            pass\n",
    "    else:\n",
    "        print(f\"File is already present at {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools\n",
    "\n",
    "with open(\"README.md\",\"r\",encoding = \"utf-8\") as f:\n",
    "    DESC = f.read()\n",
    "\n",
    "PCK_DESC = \"Python packages for VA\"\n",
    "__version__=\"0.0.0\"\n",
    "SRC_REPO = \"VA\"\n",
    "REPO_NAME = \"MLOps-VA\"\n",
    "AUTHOR_EMAIL = \"tysonbarretto1991@gmail.com\"\n",
    "AUTHOR_NAME = \"tysonbarreto\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setuptools.setup(\n",
    "    name=SRC_REPO,\n",
    "    version=__version__,\n",
    "    author=AUTHOR_NAME,\n",
    "    author_email=AUTHOR_EMAIL,\n",
    "    description=PCK_DESC,\n",
    "    long_description=DESC,\n",
    "    long_description_content_type=\"text/markdown\",\n",
    "    url=f\"https://github.com/{AUTHOR_NAME}/{REPO_NAME}\",\n",
    "    project_urls={\n",
    "        \"Bugs Tracker\": f\"https://github.com/{AUTHOR_NAME}/{REPO_NAME}/issues\"\n",
    "    },\n",
    "    package_dir = {\"\":\"VA\"},\n",
    "    packages = setuptools.find_packages(where = \"VA\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from from_root import from_root\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#LOG_FILE_EXT = f\"{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.log\"\n",
    "\n",
    "LOG_FILE_EXT = f\"{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.log\"\n",
    "\n",
    "LOG_DIR = \"logs\"\n",
    "\n",
    "LOG_PATH = os.path.join(LOG_DIR,LOG_FILE_EXT)\n",
    "\n",
    "LOG_STR = \"[%(asctime)s] %(levelname)s - %(module)s - %(message)s\"\n",
    "\n",
    "os.makedirs(LOG_PATH, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=LOG_STR,\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.info(\"VALogger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LOG_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mLOG_DIR\u001b[49m,LOG_FILE_EXT)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LOG_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "os.path.join(LOG_DIR,LOG_FILE_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    int('Hi')\n",
    "except Exception as e:\n",
    "    x= sys.exc_info()\n",
    "    #raise e\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def error_message_detail(error, error_detail:sys):\n",
    "    _, _, exc_tb = error_detail.exc_info() # traceback\n",
    "    file_name = exc_tb.tb_frame.f_code.co_filename\n",
    "    error_message = \"Error occurred python script name [{0}] line number [{1}] error message [{2}]\".format(\n",
    "        file_name, exc_tb.tb_lineno, str(error)\n",
    "    )\n",
    "\n",
    "    return error_message\n",
    "\n",
    "class VAException(Exception):\n",
    "    def __init__(self, error_message, error_detail):\n",
    "        \"\"\"\n",
    "        :param error_message: error message in string format\n",
    "        \"\"\"\n",
    "        super().__init__(error_message)\n",
    "        self.error_message = error_message_detail(\n",
    "            error_message, error_detail=error_detail\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values\n",
    "import pymongo\n",
    "import certifi\n",
    "ca=certifi.where()\n",
    "x= dotenv_values(\"notebook/KEY.env\").values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values(['mongodb+srv://tysonbarretto1991:k2IfY81hLF4gscVU@cluster0.bp8lc.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnxn_string = x\n",
    "client_ = pymongo.MongoClient(cnxn_string,  tlsCAFile=ca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VA import MONGODB_URL_KEY, COLLECTION_NAME, DATABASE_NAME, logger, Dataset\n",
    "from VA.exception import VAException\n",
    "from typing import OrderedDict, Any\n",
    "from dataclasses import dataclass, field\n",
    "from dotenv import dotenv_values\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "\n",
    "import certifi\n",
    "\n",
    "ca = certifi.where()\n",
    "\n",
    "@dataclass\n",
    "class MongoDBClient:\n",
    "    mongo_db_key: str = MONGODB_URL_KEY\n",
    "    collection_name: str = COLLECTION_NAME\n",
    "    database_name: str = DATABASE_NAME\n",
    "    \n",
    "    def __post_init__(self)->None:\n",
    "        try:\n",
    "            env_ = dotenv_values(self.mongo_db_key)\n",
    "            cnxn_string = env_.values()\n",
    "            self.client = pymongo.MongoClient(cnxn_string,  tlsCAFile=ca)\n",
    "            self.database = self.client[self.database_name]\n",
    "            self.collection = self.database[self.collection_name]\n",
    "            logger.info(\"Connection with MongoDB established!\")\n",
    "        except Exception as e:\n",
    "            logger.info(VAException(e, sys))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-09-11 22:31:46,580 ] VALogger - INFO - Connection with MongoDB established!\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    mongo_client: MongoDBClient = MongoDBClient()\n",
    "      \n",
    "    def export_collection_as_dataframe(self)->pd.DataFrame:\n",
    "        try:\n",
    "\n",
    "            collection = self.mongo_client.collection\n",
    "\n",
    "            df = pd.DataFrame(list(collection.find()))\n",
    "            if \"_id\" in df.columns.to_list():\n",
    "                df = df.drop(columns=[\"_id\"], axis=1)\n",
    "            df.replace({\"na\":np.nan},inplace=True)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise VAException(e,sys)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-09-11 21:38:12,282 ] VALogger - INFO - Connection with MongoDB established!\n"
     ]
    }
   ],
   "source": [
    "MongoDBClient = MongoDBClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>continent</th>\n",
       "      <th>education_of_employee</th>\n",
       "      <th>has_job_experience</th>\n",
       "      <th>requires_job_training</th>\n",
       "      <th>no_of_employees</th>\n",
       "      <th>yr_of_estab</th>\n",
       "      <th>region_of_employment</th>\n",
       "      <th>prevailing_wage</th>\n",
       "      <th>unit_of_wage</th>\n",
       "      <th>full_time_position</th>\n",
       "      <th>case_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EZYV01</td>\n",
       "      <td>Asia</td>\n",
       "      <td>High School</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>14513</td>\n",
       "      <td>2007</td>\n",
       "      <td>West</td>\n",
       "      <td>592.2029</td>\n",
       "      <td>Hour</td>\n",
       "      <td>Y</td>\n",
       "      <td>Denied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EZYV02</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>2412</td>\n",
       "      <td>2002</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>83425.6500</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EZYV03</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>44444</td>\n",
       "      <td>2008</td>\n",
       "      <td>West</td>\n",
       "      <td>122996.8600</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>Denied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EZYV04</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>98</td>\n",
       "      <td>1897</td>\n",
       "      <td>West</td>\n",
       "      <td>83434.0300</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>Denied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EZYV05</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>1082</td>\n",
       "      <td>2005</td>\n",
       "      <td>South</td>\n",
       "      <td>149907.3900</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25475</th>\n",
       "      <td>EZYV25476</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>2601</td>\n",
       "      <td>2008</td>\n",
       "      <td>South</td>\n",
       "      <td>77092.5700</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25476</th>\n",
       "      <td>EZYV25477</td>\n",
       "      <td>Asia</td>\n",
       "      <td>High School</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3274</td>\n",
       "      <td>2006</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>279174.7900</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25477</th>\n",
       "      <td>EZYV25478</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>1121</td>\n",
       "      <td>1910</td>\n",
       "      <td>South</td>\n",
       "      <td>146298.8500</td>\n",
       "      <td>Year</td>\n",
       "      <td>N</td>\n",
       "      <td>Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25478</th>\n",
       "      <td>EZYV25479</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1918</td>\n",
       "      <td>1887</td>\n",
       "      <td>West</td>\n",
       "      <td>86154.7700</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25479</th>\n",
       "      <td>EZYV25480</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3195</td>\n",
       "      <td>1960</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>70876.9100</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>Certified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25480 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         case_id continent education_of_employee has_job_experience  \\\n",
       "0         EZYV01      Asia           High School                  N   \n",
       "1         EZYV02      Asia              Master's                  Y   \n",
       "2         EZYV03      Asia            Bachelor's                  N   \n",
       "3         EZYV04      Asia            Bachelor's                  N   \n",
       "4         EZYV05    Africa              Master's                  Y   \n",
       "...          ...       ...                   ...                ...   \n",
       "25475  EZYV25476      Asia            Bachelor's                  Y   \n",
       "25476  EZYV25477      Asia           High School                  Y   \n",
       "25477  EZYV25478      Asia              Master's                  Y   \n",
       "25478  EZYV25479      Asia              Master's                  Y   \n",
       "25479  EZYV25480      Asia            Bachelor's                  Y   \n",
       "\n",
       "      requires_job_training  no_of_employees  yr_of_estab  \\\n",
       "0                         N            14513         2007   \n",
       "1                         N             2412         2002   \n",
       "2                         Y            44444         2008   \n",
       "3                         N               98         1897   \n",
       "4                         N             1082         2005   \n",
       "...                     ...              ...          ...   \n",
       "25475                     Y             2601         2008   \n",
       "25476                     N             3274         2006   \n",
       "25477                     N             1121         1910   \n",
       "25478                     Y             1918         1887   \n",
       "25479                     N             3195         1960   \n",
       "\n",
       "      region_of_employment  prevailing_wage unit_of_wage full_time_position  \\\n",
       "0                     West         592.2029         Hour                  Y   \n",
       "1                Northeast       83425.6500         Year                  Y   \n",
       "2                     West      122996.8600         Year                  Y   \n",
       "3                     West       83434.0300         Year                  Y   \n",
       "4                    South      149907.3900         Year                  Y   \n",
       "...                    ...              ...          ...                ...   \n",
       "25475                South       77092.5700         Year                  Y   \n",
       "25476            Northeast      279174.7900         Year                  Y   \n",
       "25477                South      146298.8500         Year                  N   \n",
       "25478                 West       86154.7700         Year                  Y   \n",
       "25479              Midwest       70876.9100         Year                  Y   \n",
       "\n",
       "      case_status  \n",
       "0          Denied  \n",
       "1       Certified  \n",
       "2          Denied  \n",
       "3          Denied  \n",
       "4       Certified  \n",
       "...           ...  \n",
       "25475   Certified  \n",
       "25476   Certified  \n",
       "25477   Certified  \n",
       "25478   Certified  \n",
       "25479   Certified  \n",
       "\n",
       "[25480 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset().export_collection_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=Dataset().export_collection_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    MongoDBClient:MongoDBClient\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.database_name = self.MongoDBClient.database_name\n",
    "        self.collection_name = self.MongoDBClient.collection_name\n",
    "        self.client = self.MongoDBClient.client()\n",
    "\n",
    "    def get_data(self):\n",
    "        database = self.client[self.database_name]\n",
    "        records = database[self.collection_name]\n",
    "        records = self.collection_name.find()\n",
    "        df = pd.DataFrame(list(records))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VA.logger import logger\n",
    "from VA.exception import VAException\n",
    "from VA.entity import Dataset\n",
    "from VA.config import Config, MongoDBClient\n",
    "from VA.components import DataIngestion\n",
    "import sys\n",
    "from datetime import date\n",
    "from scipy.stats import skew\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, precision_score, classification_report, ConfusionMatrixDisplay, recall_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from imblearn.combine import SMOTETomek,SMOTEENN\n",
    "from copy import deepcopy as dc\n",
    "#-------------------------------PIPELINE-----------------------#\n",
    "\n",
    "\n",
    "#DataFrame Loaded and saved locally\n",
    "\n",
    "# data_ingestion = DataIngestion(mongo_client=MongoDBClient(), params_config=Config())\n",
    "\n",
    "# df=data_ingestion.get_data()\n",
    "\n",
    "# train_set, test_set = data_ingestion.train_test_split_(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [feature for feature in df.columns if df[feature].dtype=='O']\n",
    "numerical_features = [feature for feature in df.columns if df[feature].dtype!='O']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('case_status', axis=1)\n",
    "y = df['case_status']\n",
    "y = np.where(y=='Denied',1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['company_age'] = date.today().year - X['yr_of_estab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer(method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_features = ['company_age', 'no_of_employees']\n",
    "X_copy = pt.fit_transform(X[transform_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = pd.DataFrame(X_copy, columns=transform_features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_columns = ['has_job_experience','requires_job_training','full_time_position','education_of_employee']\n",
    "oh_columns = ['continent','unit_of_wage','region_of_employment']\n",
    "transform_columns = transform_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "oh_encoder = OneHotEncoder()\n",
    "or_encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipline=Pipeline(\n",
    "    steps=[\n",
    "        ('transformer', PowerTransformer(method='yeo-johnson'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('OneHotEncoder', oh_encoder, oh_columns),\n",
    "        ('OrdinalEncoder', or_encoder ,or_columns),\n",
    "        ('StandardScaler', scaler, numerical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTEENN(random_state=42, sampling_strategy='minority')\n",
    "X_res, y_res = smt.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(actual, preds):\n",
    "    acc = accuracy_score(actual,preds)\n",
    "    f1 = f1_score(actual,preds)\n",
    "    roc = roc_auc_score(actual,preds)\n",
    "    precision = precision_score(actual,preds)\n",
    "    recall = recall_score(actual,preds)\n",
    "    return acc, f1, roc, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RandomForest\":RandomForestClassifier(),\n",
    "    \"GradientBoosting\":GradientBoostingClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def evaluate(X:np.array,y:np.array,models:dict):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    models_list = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        acc, f1, roc, precision, recall = calc_accuracy(y_test, y_test_pred)\n",
    "        \n",
    "        models_list.append({\n",
    "            'model_name':model_name,\"model\":model,\"acc\":acc, \"f1\":f1, \"roc\":roc, \"precision\":precision, \"recall\":recall\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name:>20}\\n {'*'*30}\\n- Accuray: {acc:.4f}\\n- F1 score: {f1:.4f}\\n- Roc Auc: {roc:.4f}\\n- Precision:{precision:.4f}\\n- Recall:{recall:.4f}\\n {'*'*30}\\n {'*'*30}\")\n",
    "\n",
    "    report_df = pd.DataFrame(models_list)\n",
    "    return report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        RandomForest\n",
      " ******************************\n",
      "- Accuray: 0.9570\n",
      "- F1 score: 0.9599\n",
      "- Roc Auc: 0.9564\n",
      "- Precision:0.9553\n",
      "- Recall:0.9645\n",
      " ******************************\n",
      " ******************************\n",
      "    GradientBoosting\n",
      " ******************************\n",
      "- Accuray: 0.9007\n",
      "- F1 score: 0.9063\n",
      "- Roc Auc: 0.9007\n",
      "- Precision:0.9125\n",
      "- Recall:0.9002\n",
      " ******************************\n",
      " ******************************\n"
     ]
    }
   ],
   "source": [
    "models_df = evaluate(X=X_res, y=y_res, models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={\n",
    "    \"RandomForest\":{\n",
    "        \"max_depth\":[10,12,None,15,20],\n",
    "        \"max_features\": ['sqrt', 'log2', None],\n",
    "        \"n_estimators\": [10,50,100,200]\n",
    "    },\n",
    "    \"GradientBoosting\":{\n",
    "        \"max_depth\":[10,12,None,15,20],\n",
    "        \"max_features\": ['sqrt', 'log2', None],\n",
    "        \"n_estimators\": [10,50,100,200]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RandomForest': {'max_depth': [10, 12, None, 15, 20],\n",
       "  'max_features': ['sqrt', 'log2', None],\n",
       "  'n_estimators': [10, 50, 100, 200]},\n",
       " 'GradientBoosting': {'max_depth': [10, 12, None, 15, 20],\n",
       "  'max_features': ['sqrt', 'log2', None],\n",
       "  'n_estimators': [10, 50, 100, 200]}}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for index,row in models_df.iterrows():\n",
    "#     print(model_params.get(row.model_name))\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tyson.Barreto.AAT\\AppData\\Local\\anaconda3\\envs\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 60 is smaller than n_iter=100. Running 60 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tyson.Barreto.AAT\\AppData\\Local\\anaconda3\\envs\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 60 is smaller than n_iter=100. Running 60 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    }
   ],
   "source": [
    "best_params = {}\n",
    "for index,row in models_df.iterrows():\n",
    "    random = RandomizedSearchCV(\n",
    "        estimator=row.model,\n",
    "        param_distributions=model_params.get(row.model_name),\n",
    "        n_iter=100,\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    random.fit(X_res, y_res)\n",
    "    best_params.update(\n",
    "        {\n",
    "            row.model_name:random.best_params_\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RandomForest': {'n_estimators': 100,\n",
       "  'max_features': 'log2',\n",
       "  'max_depth': None},\n",
       " 'GradientBoosting': {'n_estimators': 200,\n",
       "  'max_features': 'log2',\n",
       "  'max_depth': 20}}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-09-12 12:29:21,225 ] VALogger - INFO - Connection with MongoDB established!\n",
      "[ 2024-09-12 12:29:22,837 ] VALogger - INFO - raw_dataset saved in arifacts\\raw successfully\n",
      "[ 2024-09-12 12:29:22,839 ] VALogger - INFO - Raw df is loaded successfully!\n",
      "[ 2024-09-12 12:29:22,933 ] VALogger - INFO - train_dataset saved in arifacts\\processed successfully\n",
      "[ 2024-09-12 12:29:22,969 ] VALogger - INFO - test_dataset saved in arifacts\\processed successfully\n",
      "[ 2024-09-12 12:29:22,971 ] VALogger - INFO - train set and test set are loaded successfully!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m df\u001b[38;5;241m=\u001b[39mdata_ingestion\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[0;32m     17\u001b[0m train_set, test_set \u001b[38;5;241m=\u001b[39m data_ingestion\u001b[38;5;241m.\u001b[39mtrain_test_split_(df\u001b[38;5;241m=\u001b[39mdf)\n\u001b[1;32m---> 19\u001b[0m apply_transformation \u001b[38;5;241m=\u001b[39m \u001b[43mDataTransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(apply_transformation))\n",
      "File \u001b[1;32m<string>:4\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, train_dataset)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Tyson.Barreto.AAT\\OneDrive - Association of Accounting Technicians\\Desktop\\Learning\\Learning\\MLOps\\MLOps-VA\\VA\\components\\data_transformation.py:41\u001b[0m, in \u001b[0;36mDataTransformation.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_features \u001b[38;5;241m=\u001b[39m [feature \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[feature]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumerical_features \u001b[38;5;241m=\u001b[39m [feature \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[feature]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from VA.logger import logger\n",
    "from VA.exception import VAException\n",
    "from VA.components import DataTransformation, DataIngestion\n",
    "from VA.entity import Dataset\n",
    "from VA.config import MongoDBClient, Config\n",
    "import sys\n",
    "\n",
    "#-------------------------------PIPELINE-----------------------#\n",
    "\n",
    "\n",
    "#DataFrame Loaded and saved locally\n",
    "\n",
    "data_ingestion = DataIngestion(mongo_client=MongoDBClient(), params_config=Config())\n",
    "\n",
    "df=data_ingestion.get_data()\n",
    "\n",
    "train_set, test_set = data_ingestion.train_test_split_(df=df)\n",
    "\n",
    "apply_transformation = DataTransformation(train_set)\n",
    "\n",
    "print(len(apply_transformation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-09-12 13:08:31,878 ] VALogger - INFO - Connection with MongoDB established!\n",
      "[ 2024-09-12 13:08:33,659 ] VALogger - INFO - raw_dataset saved in arifacts\\raw successfully\n",
      "[ 2024-09-12 13:08:33,660 ] VALogger - INFO - Raw df is loaded successfully!\n",
      "[ 2024-09-12 13:08:33,758 ] VALogger - INFO - train_dataset saved in arifacts\\processed successfully\n",
      "[ 2024-09-12 13:08:33,800 ] VALogger - INFO - test_dataset saved in arifacts\\processed successfully\n",
      "[ 2024-09-12 13:08:33,802 ] VALogger - INFO - train set and test set are loaded successfully!\n",
      "[ 2024-09-12 13:08:33,964 ] VALogger - INFO - preprocessor has been loaded and applied to the dataset successfully\n"
     ]
    }
   ],
   "source": [
    "from VA.logger import logger\n",
    "from VA.exception import VAException\n",
    "from VA.components import DataTransformation, DataIngestion, ModelEvaluation\n",
    "from VA.entity import Dataset\n",
    "from VA.config import MongoDBClient, Config\n",
    "import sys\n",
    "\n",
    "#-------------------------------PIPELINE-----------------------#\n",
    "\n",
    "\n",
    "#DataFrame Loaded and saved locally\n",
    "\n",
    "data_ingestion = DataIngestion(mongo_client=MongoDBClient(), params_config=Config())\n",
    "\n",
    "df=data_ingestion.get_data()\n",
    "\n",
    "train_set, test_set = data_ingestion.train_test_split_(df=df)\n",
    "\n",
    "X_res, y_res = DataTransformation(train_set).apply_transformation()\n",
    "\n",
    "model_eval = ModelEvaluation(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>\n"
     ]
    }
   ],
   "source": [
    "for mn, m in model_eval.models.items():\n",
    "    print(m.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Any\n",
    "import os\n",
    "def save_load_pickle(filepath:str, data:Any=None, save:bool=False, load:bool=False):\n",
    "    \n",
    "    if save:\n",
    "        with open(filepath, 'wb') as handle:\n",
    "            pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        filename= os.path.split(filepath)[-1].split('.')[0]\n",
    "        filepath = os.path.split(os.path.relpath(filepath))[0]\n",
    "        logger.info(f\"{filename} saved in {filepath} successfully\")\n",
    "        \n",
    "    elif load:\n",
    "        with open(filepath, 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "        filename= os.path.split(filepath)[-1].split('.')[0]\n",
    "        filepath = os.path.split(os.path.relpath(filepath))[0]\n",
    "        logger.info(f\"{filename} saved in {filepath} successfully\")\n",
    "        return data\n",
    "    elif (save==True) & (load==True):\n",
    "        logger.info(f\"either save or load at one time\")\n",
    "        raise ValueError(\"either save or load at one time\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-09-12 13:46:32,052 ] VALogger - INFO - best_params saved in arifacts\\params successfully\n"
     ]
    }
   ],
   "source": [
    "data = save_load_pickle(r'arifacts\\params\\best_params.pkl',load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.sort_values('acc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "A load persistent id instruction was encountered,\nbut no persistent_load function was specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#-------------------------------PIPELINE-----------------------#\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# reports_df = model_eval.evaluate()\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# best_params_dict = model_eval.RGS_CV\u001b[39;00m\n\u001b[0;32m     26\u001b[0m perdictor \u001b[38;5;241m=\u001b[39m VAPredcitor(model_path\u001b[38;5;241m=\u001b[39mRAW_DATASET_PATH, df_path\u001b[38;5;241m=\u001b[39mBEST_PARAMS_DATASET_PATH)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mperdictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tyson.Barreto.AAT\\OneDrive - Association of Accounting Technicians\\Desktop\\Learning\\Learning\\MLOps\\MLOps-VA\\VA\\components\\model_predictor.py:40\u001b[0m, in \u001b[0;36mVAPredcitor.predict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     models_df \u001b[38;5;241m=\u001b[39m \u001b[43msave_load_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     df \u001b[38;5;241m=\u001b[39m load_parquet(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_path)\n\u001b[0;32m     42\u001b[0m     X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_status\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tyson.Barreto.AAT\\OneDrive - Association of Accounting Technicians\\Desktop\\Learning\\Learning\\MLOps\\MLOps-VA\\VA\\utils\\utils.py:24\u001b[0m, in \u001b[0;36msave_load_pickle\u001b[1;34m(filepath, data, save, load)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m load:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m---> 24\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     filename\u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(filepath)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrelpath(filepath))[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: A load persistent id instruction was encountered,\nbut no persistent_load function was specified."
     ]
    }
   ],
   "source": [
    "from VA.logger import logger\n",
    "from VA.exception import VAException\n",
    "from VA.components import DataTransformation, DataIngestion, ModelEvaluation, VAPredcitor\n",
    "from VA.constants import RAW_DATASET_PATH, BEST_PARAMS_DATASET_PATH, TEST_SET_PATH\n",
    "from VA.utils import load_parquet\n",
    "from VA.entity import Dataset\n",
    "from VA.config import MongoDBClient, Config\n",
    "import sys\n",
    "from from_root import from_root\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "#-------------------------------PIPELINE-----------------------#\n",
    "\n",
    "\n",
    "#DataFrame Loaded and saved locally\n",
    "\n",
    "# data_ingestion = DataIngestion(mongo_client=MongoDBClient(), params_config=Config())\n",
    "# df=data_ingestion.get_data()\n",
    "# train_set, test_set = data_ingestion.train_test_split_(df=df)\n",
    "# X_res, y_res = DataTransformation(train_set).apply_transformation()\n",
    "# model_eval = ModelEvaluation(X_res, y_res)\n",
    "# reports_df = model_eval.evaluate()\n",
    "# best_params_dict = model_eval.RGS_CV\n",
    "\n",
    "perdictor = VAPredcitor(model_path=RAW_DATASET_PATH, df_path=BEST_PARAMS_DATASET_PATH)\n",
    "perdictor.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
